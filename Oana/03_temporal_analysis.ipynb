{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Temporal Analysis (HDFS Version)\n",
    "This notebook analyzes genre growth trends from 2010 to 2025 using HDFS data.\n",
    "It also analyzes the popularity of different **Reading Formats** (E-Book, Audio, etc.) over time.\n",
    "\n",
    "**Update**: Analysis is based on **Read Date** (`read_at`), not Publication Year.\n",
    "**Optimization**: Processing is split into 5-year intervals to manage memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, year, when, to_timestamp\n",
    "\n",
    "# Set Plotting Style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Configurable Limit (Set to 0 for full dataset)\n",
    "LIMIT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/14 23:39:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session created.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark (Cluster Mode)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Goodreads_EDA_Temporal\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS Paths\n",
    "hdfs_base = \"hdfs:///user/ubuntu/goodreads_data/processed\"\n",
    "interactions_path = f\"{hdfs_base}/master_interactions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Reading master_interactions from HDFS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Read Master Interactions\n",
    "print(\"â³ Reading master_interactions from HDFS...\")\n",
    "try:\n",
    "    df = spark.read.parquet(interactions_path)\n",
    "    if LIMIT > 0:\n",
    "        df = df.limit(LIMIT)\n",
    "    \n",
    "    # Parse Dates Globally (this is a narrow transformation, cheap)\n",
    "    df_read = df.filter((col(\"is_read\") == 1) & (col(\"read_at\").isNotNull()))\n",
    "    df_parsed = df_read.withColumn(\"read_date\", to_timestamp(col(\"read_at\"), \"EEE MMM dd HH:mm:ss Z yyyy\"))\n",
    "    df_with_year = df_parsed.withColumn(\"read_year\", year(col(\"read_date\")))\n",
    "    \n",
    "    # Persist the base dataframe with years to avoid re-reading/re-parsing for each interval\n",
    "    # Using a cache might be heavy if dataset is huge, but filtering `is_read` reduces it significantly.\n",
    "    # df_with_year.cache()\n",
    "    print(f\"âœ… Data loaded.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_genre_interval(df_base, start_year, end_year):\n",
    "    print(f\"\\nâ³ Processing Interval: {start_year}-{end_year}\")\n",
    "    \n",
    "    # Filter for Interval\n",
    "    df_filtered = df_base.filter(\n",
    "        (col(\"read_year\") >= start_year) & \n",
    "        (col(\"read_year\") <= end_year)\n",
    "    )\n",
    "    \n",
    "    # Explode Shelves (Expensive Step)\n",
    "    df_exploded = df_filtered.select(\n",
    "        col(\"read_year\"), \n",
    "        explode(col(\"popular_shelves\")).alias(\"shelf\")\n",
    "    )\n",
    "    \n",
    "    df_raw_shelves = df_exploded.select(\n",
    "        col(\"read_year\"),\n",
    "        col(\"shelf.name\").alias(\"shelf_name\")\n",
    "    )\n",
    "    \n",
    "    # Filter Instructions\n",
    "    ignore_shelves = [\n",
    "        'to-read', 'currently-reading', 'favorites', 'books-i-own', \n",
    "        'kindle', 'owned', 'ebook', 'library', 'to-buy', 'owned-books', \n",
    "        'audio', 'audiobook', 'ebooks', 'audiobooks', 'default', 'wish-list', \n",
    "        'my-books', 'read', 'borrowed', 'book-club', 'bookclub', 'book-group',\n",
    "        'read-in-2010', 'read-in-2011', 'read-in-2012', 'read-in-2013', 'read-in-2014', 'read-in-2015',\n",
    "        'read-2010', 'read-2011', 'read-2012', 'read-2013', 'read-2014', 'read-2015',\n",
    "        'dnf', 'did-not-finish', 'abandoned', 'unfinished', 'stopped-reading', 'didn-t-finish',\n",
    "        'adult', 'adult-fiction', 'novels', 'novel', 'books', 'hardcover', 'paperback',\n",
    "        'series', 'trilogy', 'standalone', 're-read', 'reread', 'all-time-favorites',\n",
    "        '5-stars', '4-stars', 'recommended', 'reviewed', 'netgalley', 'arc',\n",
    "        'kindle-books', 'nook', 'audible', 'calibre', 'shelfari-favorites', 'favorites', 'favourites',\n",
    "        'owned-tbr', 'tbr', 'to-be-read', 'my-library', 'library-books', \n",
    "        'free', 'freebie', 'giveaways', 'kindle-freebie', 'listened-to',\n",
    "        'english', 'fiction', 'general-fiction', 'literature', 'e-book', 'e-books', 'maybe',\n",
    "        'own-it', 'on-hold', 'contemporary', 'i-own', 'american', 'favorite'\n",
    "    ]\n",
    "\n",
    "    df_genres = df_raw_shelves.filter(~col(\"shelf_name\").isin(ignore_shelves))\n",
    "    df_genres = df_genres.filter(~col(\"shelf_name\").contains(\"read-in\"))\n",
    "    df_genres = df_genres.filter(~col(\"shelf_name\").contains(\"read-20\"))\n",
    "    df_genres = df_genres.filter(~col(\"shelf_name\").contains(\"to-read\"))\n",
    "    df_genres = df_genres.filter(~col(\"shelf_name\").contains(\"challenge\"))\n",
    "    df_genres = df_genres.filter(~col(\"shelf_name\").contains(\"kindle\"))\n",
    "    df_genres = df_genres.filter(~col(\"shelf_name\").contains(\"audio\"))\n",
    "    df_genres = df_genres.filter(~col(\"shelf_name\").contains(\"owned\"))\n",
    "\n",
    "    # Normalize\n",
    "    df_genres = df_genres.withColumn(\"genre\", \n",
    "        when(col(\"shelf_name\") == \"nonfiction\", \"non-fiction\")\n",
    "        .otherwise(col(\"shelf_name\"))\n",
    "    )\n",
    "\n",
    "    # Aggregate\n",
    "    genre_counts = df_genres.groupBy(\"read_year\", \"genre\").count().orderBy(\"read_year\")\n",
    "    \n",
    "    # Collect\n",
    "    print(f\"   Collecting results for {start_year}-{end_year}...\")\n",
    "    pdf_trends = genre_counts.toPandas()\n",
    "    \n",
    "    # Save\n",
    "    filename = f\"genre_trends_{start_year}_{end_year}.csv\"\n",
    "    if not pdf_trends.empty:\n",
    "        pdf_trends.to_csv(filename, index=False)\n",
    "        print(f\"âœ… Saved {filename}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ No data for {start_year}-{end_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Loop\n",
    "Running the analysis in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â³ Processing Interval: 2010-2015\n",
      "   Collecting results for 2010-2015...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved genre_trends_2010_2015.csv\n",
      "\n",
      "â³ Processing Interval: 2015-2020\n",
      "   Collecting results for 2015-2020...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved genre_trends_2015_2020.csv\n",
      "\n",
      "â³ Processing Interval: 2020-2025\n",
      "   Collecting results for 2020-2025...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved genre_trends_2020_2025.csv\n",
      "\n",
      "ğŸ‰ All intervals processed.\n"
     ]
    }
   ],
   "source": [
    "intervals = [\n",
    "    (2010, 2015),\n",
    "    (2015, 2020),\n",
    "    (2020, 2025)\n",
    "]\n",
    "\n",
    "for start, end in intervals:\n",
    "    process_genre_interval(df_with_year, start, end)\n",
    "    \n",
    "print(\"\\nğŸ‰ All intervals processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
