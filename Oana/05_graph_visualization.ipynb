{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Graph Visualization (HDFS Version)\n",
    "This notebook visualizes the book communities detected in Phase 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc, collect_list, struct\n",
    "\n",
    "# Configurable Limit for Visualization\n",
    "TOP_COMMUNITIES = 10\n",
    "NODES_PER_COMMUNITY = 25 # Reduced sample size per user request\n",
    "MAX_BOOKS_PER_USER_FILTER = 500 # Filter out noisy heavy readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Goodreads_Graph_Vis\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS Paths\n",
    "hdfs_base = \"hdfs:///user/ubuntu/goodreads_data/processed\"\n",
    "communities_path = f\"{hdfs_base}/book_communities\"\n",
    "books_path = f\"{hdfs_base}/master_books\" # Need books metadata for titles\n",
    "interactions_path = f\"{hdfs_base}/master_interactions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Communities\n",
    "print(\"‚è≥ Reading communities...\")\n",
    "try:\n",
    "    df_communities = spark.read.parquet(communities_path)\n",
    "    print(f\"‚úÖ Communities loaded. Rows: {df_communities.count()}\")\n",
    "    df_communities.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Communities file not found: {e}\")\n",
    "\n",
    "# Load Books Metadata (for Titles)\n",
    "print(\"‚è≥ Reading books metadata...\")\n",
    "try:\n",
    "    df_books = spark.read.json(f\"hdfs:///user/ubuntu/goodreads_books.json.gz\").select(\"book_id\", \"title\", \"title_without_series\")\n",
    "    # Note: Using raw books for titles if master_books doesn't exist or has different schema\n",
    "    print(\"‚úÖ Books metadata loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Books file not found: {e}\")\n",
    "\n",
    "# Load Interactions & Filter for Community Popularity\n",
    "print(\"‚è≥ Reading interactions...\")\n",
    "try:\n",
    "    df_interactions = spark.read.parquet(interactions_path)\n",
    "    \n",
    "    # Filter heavy users (same logic as Graph Construction)\n",
    "    user_counts = df_interactions.groupBy(\"user_id\").count()\n",
    "    valid_users = user_counts.filter(col(\"count\") <= MAX_BOOKS_PER_USER_FILTER).select(\"user_id\")\n",
    "    \n",
    "    df_filtered_interactions = df_interactions.join(valid_users, \"user_id\", \"inner\")\n",
    "    \n",
    "    # Count readers per book (Filtered / Local Popularity)\n",
    "    df_book_counts = df_filtered_interactions.groupBy(\"book_id\").count().withColumnRenamed(\"count\", \"read_count\")\n",
    "    print(\"‚úÖ Interactions filtered (heavy users removed) and aggregated.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Interactions file not found: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Communities with Titles AND Popularity\n",
    "df_communities = df_communities.withColumnRenamed(\"id\", \"book_id\")\n",
    "\n",
    "# 1. Join with Titles\n",
    "df_enriched = df_communities.join(df_books, \"book_id\")\n",
    "\n",
    "# 2. Join with Read Counts (Popularity)\n",
    "df_enriched = df_enriched.join(df_book_counts, \"book_id\", \"left\").fillna(0, subset=[\"read_count\"])\n",
    "\n",
    "# Find Top Communities\n",
    "top_communities = df_enriched.groupBy(\"component\").count().orderBy(desc(\"count\")).limit(TOP_COMMUNITIES)\n",
    "top_ids = [row[\"component\"] for row in top_communities.collect()]\n",
    "print(f\"üîπ Top {TOP_COMMUNITIES} Communities by size: {top_ids}\")\n",
    "\n",
    "# Filter Data for Visualization\n",
    "# FOCUS ON TOP 1 COMMUNITY ONLY per user request\n",
    "if top_ids:\n",
    "    top_1_id = top_ids[0]\n",
    "    print(f\"üîπ Focusing Visualization on Top 1 Community Only (ID: {top_1_id})\")\n",
    "    df_vis = df_enriched.filter(col(\"component\") == top_1_id)\n",
    "else:\n",
    "    df_vis = df_enriched.filter(col(\"component\").isin(top_ids))\n",
    "\n",
    "# Collect samples for Gephi/NetworkX\n",
    "# We rankings books by POPULARITY (read_count) now, not just ID\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# RANK BY READ COUNT (Most popular books first)\n",
    "windowSpec = Window.partitionBy(\"component\").orderBy(desc(\"read_count\")) \n",
    "\n",
    "df_sample = df_vis.withColumn(\"rank\", row_number().over(windowSpec)).filter(col(\"rank\") <= NODES_PER_COMMUNITY)\n",
    "\n",
    "pdf_sample = df_sample.toPandas()\n",
    "print(f\"üîπ Sample Data for Vis: {len(pdf_sample)} rows\")\n",
    "print(pdf_sample[['title', 'read_count', 'rank']].head())\n",
    "\n",
    "# Save Top 1 Community to Local CSV for Inspection\n",
    "if top_ids:\n",
    "    top_comm_id = top_ids[0]\n",
    "    print(f\"üîπ Extracting books for Top Community ID: {top_comm_id}\")\n",
    "    \n",
    "    # Order export by read_count too\n",
    "    df_top1 = df_enriched.filter(col(\"component\") == top_comm_id) \\\n",
    "        .select(\"book_id\", \"title\", \"read_count\", \"component\") \\\n",
    "        .orderBy(desc(\"read_count\"))\n",
    "    \n",
    "    count_top1 = df_top1.count()\n",
    "    print(f\"üîπ Found {count_top1} books in Community {top_comm_id}\")\n",
    "    \n",
    "    if count_top1 > 0:\n",
    "        df_top1.toPandas().to_csv(\"top_community_books.csv\", index=False)\n",
    "        print(f\"üíæ Saved {count_top1} books from Community {top_comm_id} to 'top_community_books.csv'\")\n",
    "        print(\"Preview of exported books (Top Popular):\")\n",
    "        df_top1.show(5, truncate=False)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Warning: Top community has 0 books after filtering (check metadata joining).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization using NetworkX\n",
    "if not pdf_sample.empty:\n",
    "    G = nx.Graph()\n",
    "    import numpy as np # Import locally for scale\n",
    "    \n",
    "    # Add nodes\n",
    "    for _, row in pdf_sample.iterrows():\n",
    "        G.add_node(row['book_id'], title=row['title'], group=row['component'], popularity=row['read_count'])\n",
    "\n",
    "    # COMPUTE EDGES FOR SAMPLE (Improve Cluster View)\n",
    "    print(\"‚è≥ Computing edges for the sample visualization...\")\n",
    "    sample_book_ids = [row['book_id'] for _, row in pdf_sample.iterrows()]\n",
    "    \n",
    "    # Read interactions for these books only\n",
    "    df_int = spark.read.parquet(interactions_path).select(\"user_id\", \"book_id\")\n",
    "    df_int_sample = df_int.filter(col(\"book_id\").isin(sample_book_ids))\n",
    "    \n",
    "    # Self-join to find edges (who read both books)\n",
    "    df_edges_sample = df_int_sample.alias(\"a\").join(df_int_sample.alias(\"b\"), \"user_id\") \\\n",
    "        .filter(col(\"a.book_id\") < col(\"b.book_id\")) \\\n",
    "        .groupBy(col(\"a.book_id\").alias(\"src\"), col(\"b.book_id\").alias(\"dst\")) \\\n",
    "        .count().withColumnRenamed(\"count\", \"weight\")\n",
    "        \n",
    "    pdf_edges = df_edges_sample.toPandas()\n",
    "    print(f\"‚úÖ Edges found: {len(pdf_edges)}\")\n",
    "    \n",
    "    # Add edges to Graph\n",
    "    for _, row in pdf_edges.iterrows():\n",
    "        G.add_edge(row['src'], row['dst'], weight=row['weight'])\n",
    "\n",
    "    # Drawing\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    \n",
    "    # Use Spring Layout with HIGH 'k' to force nodes apart\n",
    "    # k=1.5 is much larger than default, pushing nodes to edges to reduce 'hairball' overlap\n",
    "    pos = nx.spring_layout(G, k=1.5, weight='weight', iterations=100, seed=42)\n",
    "    \n",
    "    # Logarithmic Node Size to prevent one giant node from hiding others\n",
    "    # Formula: base(300) + log(1 + popularity) * factor(300)\n",
    "    node_sizes = [300 + (np.log1p(G.nodes[n]['popularity']) * 300) for n in G.nodes()]\n",
    "    \n",
    "    # Nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=pdf_sample['component'].astype(int), cmap=plt.cm.tab20, alpha=0.9)\n",
    "    \n",
    "    # Edges (Very transparent)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.2, width=1.0)\n",
    "    \n",
    "    # Labels\n",
    "    labels = {row['book_id']: row['title'] for _, row in pdf_sample.iterrows()}\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=10, font_weight='bold', bbox=dict(facecolor='white', alpha=0.7, edgecolor='gray', boxstyle='round,pad=0.3'))\n",
    "\n",
    "    plt.title(\"Deep Dive: Top 1 Book Community (Top 25 Popular Books)\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot as an image\n",
    "    plt.savefig(\"top_community_graph.png\", dpi=300, bbox_inches='tight')\n",
    "    print(\"üíæ Saved graph image to 'top_community_graph.png'\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Export to GEXF for Gephi (Better for large graphs)\n",
    "    nx.write_gexf(G, \"book_communities.gexf\")\n",
    "    print(\"üíæ Exported graph to book_communities.gexf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
