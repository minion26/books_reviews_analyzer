{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Data Linking (Optimized HDFS)\n",
    "This notebook joins Books, Interactions, and Reviews data using HDFS with optimized reads and early limiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, broadcast, length\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, FloatType, ArrayType\n",
    "\n",
    "# Configurable Limit (Set to 0 for full dataset)\n",
    "LIMIT = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session (Cluster Mode)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Goodreads_Data_Linking\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS Paths\n",
    "hdfs_base = \"hdfs:///user/ubuntu\"\n",
    "processed_dir = f\"{hdfs_base}/goodreads_data/processed\"\n",
    "\n",
    "interactions_src = f\"{hdfs_base}/goodreads_interactions_dedup.json.gz\"\n",
    "books_src = f\"{hdfs_base}/goodreads_books.json.gz\"\n",
    "reviews_src = f\"{hdfs_base}/goodreads_reviews_dedup.json.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit Schemas (CRITICAL for Performance on JSON)\n",
    "# Defining schemas prevents Spark from scanning the entire file to infer types.\n",
    "\n",
    "schema_books = StructType([\n",
    "    StructField(\"book_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"average_rating\", StringType(), True), # Often string in source, cast later\n",
    "    StructField(\"publication_year\", StringType(), True),\n",
    "    StructField(\"publisher\", StringType(), True),\n",
    "    StructField(\"popular_shelves\", ArrayType(StructType([\n",
    "        StructField(\"count\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "schema_interactions = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"book_id\", StringType(), True),\n",
    "    StructField(\"is_read\", BooleanType(), True),\n",
    "    StructField(\"rating\", IntegerType(), True)\n",
    "    # Ignoring other fields for speed\n",
    "])\n",
    "\n",
    "schema_reviews = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"book_id\", StringType(), True),\n",
    "    StructField(\"review_text\", StringType(), True),\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"n_votes\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Books\n",
    "print(\"â³ Reading Books from HDFS...\")\n",
    "df_books = spark.read.schema(schema_books).json(books_src).select(\n",
    "    col(\"book_id\"), \n",
    "    col(\"title\"), \n",
    "    col(\"average_rating\").cast(\"float\"),\n",
    "    col(\"publication_year\").cast(\"int\"), \n",
    "    col(\"publisher\"), \n",
    "    col(\"popular_shelves\")\n",
    ").filter(col(\"title\").isNotNull())\n",
    "print(\"âœ… Finished reading books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Interactions\n",
    "print(f\"â³ Reading Interactions from HDFS (Limit: {LIMIT if LIMIT > 0 else 'FULL'})...\")\n",
    "\n",
    "# Using the Schema avoids the full file scan!\n",
    "df_interactions_raw = spark.read.schema(schema_interactions).json(interactions_src)\n",
    "\n",
    "# Apply LIMIT *before* filter to strictly limit file reading (Fastest)\n",
    "df_interactions_raw = df_interactions_raw.limit(LIMIT)\n",
    "\n",
    "df_interactions = df_interactions_raw.filter(\n",
    "    (col(\"is_read\") == True) | (col(\"rating\") > 0)\n",
    ").select(\n",
    "    col(\"user_id\"), \n",
    "    col(\"book_id\"), \n",
    "    col(\"rating\").cast(\"int\"), \n",
    "    col(\"is_read\").cast(\"int\")\n",
    ")\n",
    "\n",
    "print(\"âœ… Finished Reading Interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Reviews\n",
    "print(\"â³ Reading Reviews from HDFS...\")\n",
    "df_reviews_raw = spark.read.schema(schema_reviews).json(reviews_src)\n",
    "\n",
    "# Apply LIMIT *before* filter\n",
    "if LIMIT > 0:\n",
    "    df_reviews_raw = df_reviews_raw.limit(LIMIT)\n",
    "\n",
    "df_reviews = df_reviews_raw.select(\n",
    "    col(\"user_id\"), \n",
    "    col(\"book_id\"), \n",
    "    col(\"review_text\"),\n",
    "    col(\"rating\").cast(\"int\"), \n",
    "    col(\"n_votes\").cast(\"int\"),\n",
    ").filter(length(col(\"review_text\")) > 20)\n",
    "\n",
    "print(\"âœ… Finished Reading Reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Joining Data...\")\n",
    "# Remove broadcast(df_books). The books dataset is too large (2GB+) to broadcast,\n",
    "# causing the Spark Driver to crash (OOM).\n",
    "# Spark will automatically choose SortMergeJoin or ShuffleHashJoin.\n",
    "master_interactions = df_interactions.join(df_books, on=\"book_id\", how=\"inner\")\n",
    "master_reviews = df_reviews.join(df_books, on=\"book_id\", how=\"inner\")\n",
    "\n",
    "out_inter = f\"{processed_dir}/master_interactions\"\n",
    "out_rev = f\"{processed_dir}/master_reviews\"\n",
    "    \n",
    "print(f\"ðŸ’¾ Saving to HDFS: {processed_dir}\")\n",
    "master_interactions.write.mode(\"overwrite\").parquet(out_inter)\n",
    "master_reviews.write.mode(\"overwrite\").parquet(out_rev)\n",
    "\n",
    "print(\"ðŸŽ‰ DONE! Processed data saved to HDFS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
