{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1: Data Linking (Optimized HDFS)\n",
                "This notebook joins Books, Interactions, and Reviews data using HDFS with optimized reads and early limiting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, broadcast, length\n",
                "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, FloatType, ArrayType\n",
                "\n",
                "# Configurable Limit (Set to 0 for full dataset)\n",
                "LIMIT = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
                        "26/01/14 19:55:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Spark Session created.\n"
                    ]
                }
            ],
            "source": [
                "# Initialize Spark Session (Cluster Mode)\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"Goodreads_Data_Linking\") \\\n",
                "    .config(\"spark.driver.memory\", \"8g\") \\\n",
                "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(\"âœ… Spark Session created.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# HDFS Paths\n",
                "hdfs_base = \"hdfs:///user/ubuntu\"\n",
                "processed_dir = f\"{hdfs_base}/goodreads_data/processed\"\n",
                "\n",
                "interactions_src = f\"{hdfs_base}/goodreads_interactions_dedup.json.gz\"\n",
                "books_src = f\"{hdfs_base}/goodreads_books.json.gz\"\n",
                "reviews_src = f\"{hdfs_base}/goodreads_reviews_dedup.json.gz\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explicit Schemas (CRITICAL for Performance on JSON)\n",
                "# Defining schemas prevents Spark from scanning the entire file to infer types.\n",
                "\n",
                "schema_books = StructType([\n",
                "    StructField(\"book_id\", StringType(), True),\n",
                "    StructField(\"title\", StringType(), True),\n",
                "    StructField(\"average_rating\", StringType(), True), # Often string in source, cast later\n",
                "    StructField(\"publication_year\", StringType(), True),\n",
                "    StructField(\"publisher\", StringType(), True),\n",
                "    StructField(\"popular_shelves\", ArrayType(StructType([\n",
                "        StructField(\"count\", StringType(), True),\n",
                "        StructField(\"name\", StringType(), True)\n",
                "    ])), True)\n",
                "])\n",
                "\n",
                "schema_interactions = StructType([\n",
                "    StructField(\"user_id\", StringType(), True),\n",
                "    StructField(\"book_id\", StringType(), True),\n",
                "    StructField(\"is_read\", BooleanType(), True),\n",
                "    StructField(\"rating\", IntegerType(), True),\n",
                "    StructField(\"read_at\", StringType(), True),\n",
                "    StructField(\"date_added\", StringType(), True)\n",
                "])\n",
                "\n",
                "schema_reviews = StructType([\n",
                "    StructField(\"user_id\", StringType(), True),\n",
                "    StructField(\"book_id\", StringType(), True),\n",
                "    StructField(\"review_text\", StringType(), True),\n",
                "    StructField(\"rating\", IntegerType(), True),\n",
                "    StructField(\"n_votes\", IntegerType(), True)\n",
                "])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "â³ Reading Books from HDFS...\n",
                        "âœ… Finished reading books\n"
                    ]
                }
            ],
            "source": [
                "# Read Books\n",
                "print(\"â³ Reading Books from HDFS...\")\n",
                "df_books = spark.read.schema(schema_books).json(books_src).select(\n",
                "    col(\"book_id\"), \n",
                "    col(\"title\"), \n",
                "    col(\"average_rating\").cast(\"float\"),\n",
                "    col(\"publication_year\").cast(\"int\"), \n",
                "    col(\"publisher\"), \n",
                "    col(\"popular_shelves\")\n",
                ").filter(col(\"title\").isNotNull())\n",
                "print(\"âœ… Finished reading books\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "â³ Reading Interactions from HDFS (Limit: 1000)...\n",
                        "âœ… Finished Reading Interactions\n"
                    ]
                }
            ],
            "source": [
                "# Read Interactions\n",
                "print(f\"â³ Reading Interactions from HDFS (Limit: {LIMIT if LIMIT > 0 else 'FULL'})...\")\n",
                "\n",
                "# Using the Schema avoids the full file scan!\n",
                "df_interactions_raw = spark.read.schema(schema_interactions).json(interactions_src)\n",
                "\n",
                "# Apply LIMIT *before* filter to strictly limit file reading (Fastest)\n",
                "if LIMIT > 0:\n",
                "    df_interactions_raw = df_interactions_raw.limit(LIMIT)\n",
                "\n",
                "df_interactions = df_interactions_raw.filter(\n",
                "    (col(\"is_read\") == True) | (col(\"rating\") > 0)\n",
                ").select(\n",
                "    col(\"user_id\"), \n",
                "    col(\"book_id\"), \n",
                "    col(\"rating\").cast(\"int\"), \n",
                "    col(\"is_read\").cast(\"int\"),\n",
                "    col(\"read_at\"),\n",
                "    col(\"date_added\")\n",
                ")\n",
                "\n",
                "print(\"âœ… Finished Reading Interactions\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "â³ Reading Reviews from HDFS...\n",
                        "âœ… Finished Reading Reviews\n"
                    ]
                }
            ],
            "source": [
                "# Read Reviews\n",
                "print(\"â³ Reading Reviews from HDFS...\")\n",
                "df_reviews_raw = spark.read.schema(schema_reviews).json(reviews_src)\n",
                "\n",
                "# Apply LIMIT *before* filter\n",
                "if LIMIT > 0:\n",
                "    df_reviews_raw = df_reviews_raw.limit(LIMIT)\n",
                "\n",
                "df_reviews = df_reviews_raw.select(\n",
                "    col(\"user_id\"), \n",
                "    col(\"book_id\"), \n",
                "    col(\"review_text\"),\n",
                "    col(\"rating\").cast(\"int\"), \n",
                "    col(\"n_votes\").cast(\"int\"),\n",
                ").filter(length(col(\"review_text\")) > 20)\n",
                "\n",
                "print(\"âœ… Finished Reading Reviews\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸš€ Joining Data...\n",
                        "ðŸ’¾ Saving to HDFS: hdfs:///user/ubuntu/goodreads_data/processed\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 7:>                                                          (0 + 1) / 1]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸŽ‰ DONE! Processed data saved to HDFS.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                "
                    ]
                }
            ],
            "source": [
                "print(\"ðŸš€ Joining Data...\")\n",
                "# Remove broadcast(df_books). The books dataset is too large (2GB+) to broadcast,\n",
                "# causing the Spark Driver to crash (OOM).\n",
                "# Spark will automatically choose SortMergeJoin or ShuffleHashJoin.\n",
                "master_interactions = df_interactions.join(df_books, on=\"book_id\", how=\"inner\")\n",
                "master_reviews = df_reviews.join(df_books, on=\"book_id\", how=\"inner\")\n",
                "\n",
                "out_inter = f\"{processed_dir}/master_interactions\"\n",
                "out_rev = f\"{processed_dir}/master_reviews\"\n",
                "    \n",
                "print(f\"ðŸ’¾ Saving to HDFS: {processed_dir}\")\n",
                "master_interactions.write.mode(\"overwrite\").parquet(out_inter)\n",
                "master_reviews.write.mode(\"overwrite\").parquet(out_rev)\n",
                "\n",
                "print(\"ðŸŽ‰ DONE! Processed data saved to HDFS.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}